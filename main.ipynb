{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GirIZ09-6Hk"
      },
      "outputs": [],
      "source": [
        "#Question No.1\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_regex_matches(pattern, text):\n",
        "  matches = re.findall(pattern, text)\n",
        "  print(f\"\\nMatches for pattern {pattern}\")\n",
        "  print(' '.join(matches))\n",
        "  print(\"Length: \", len(matches))"
      ],
      "metadata": {
        "id": "knQyHyGP_oZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing with a custom text\n",
        "text = \"Cat cat Hello coat cut cute Dog cart\"\n",
        "show_regex_matches(r'[a-z]+', text)\n",
        "show_regex_matches(r'[A-Z][a-z]+', text)\n",
        "show_regex_matches(r'c[aeiou]{1,2}t', text)"
      ],
      "metadata": {
        "id": "M8v9yOkOAT2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing with gutenberg text\n",
        "nltk.download('gutenberg')\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "show_regex_matches(r'[a-z]+', text)\n",
        "show_regex_matches(r'[A-Z][a-z]+', text)\n",
        "show_regex_matches(r'c[aeiou]{1,2}t', text)\n"
      ],
      "metadata": {
        "id": "CPIhYUP-BebG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question No.2\n",
        "from urllib import request\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "dIP0cXqdCUD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_from_url(url):\n",
        "  html = request.urlopen(url).read().decode('utf8')\n",
        "  return BeautifulSoup(html, 'html.parser').get_text()"
      ],
      "metadata": {
        "id": "lM4lGAweCeYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.dsu.edu.pk/contact-us/\"\n",
        "text = get_text_from_url(url)\n",
        "print(text.strip())"
      ],
      "metadata": {
        "id": "NuasWtAxDMAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question No.3\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsV-d-aXEimg",
        "outputId": "3c560c7c-ed47-4216-b2ba-35eac93f6ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_tokens(text):\n",
        "  return word_tokenize(text)"
      ],
      "metadata": {
        "id": "LNhvC0K-ErUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = get_word_tokens(text)\n",
        "print(\"Word Tokens: \", tokens)"
      ],
      "metadata": {
        "id": "-cQy_RwLEwuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting Phone Numbers from Text\n",
        "pattern = r'(\\(?\\d{3,4}\\)?[\\s\\-]?\\d{7,8}[\\s\\-]?\\d?\\d?)'\n",
        "print(\"Phone Numbers\")\n",
        "show_regex_matches(pattern, text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t2umEIk8FMr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting Emails from Text\n",
        "pattern = r'[a-zA-Z0-9._%+-]+\\@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "print(\"Emails\")\n",
        "show_regex_matches(pattern, text)"
      ],
      "metadata": {
        "id": "fHzNc9AIG591"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question No.4\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer"
      ],
      "metadata": {
        "id": "w70YLSw5IXws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The runner was running and the cats were chasing mice.\"\n",
        "tokens = get_word_tokens(text)\n",
        "print(\"Word Tokens: \", tokens)"
      ],
      "metadata": {
        "id": "vXV04aqHIiiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "porter_stemmed_tokens = [porter.stem(token) for token in tokens]\n",
        "print(\"Porter Stemmed Tokens: \", porter_stemmed_tokens)"
      ],
      "metadata": {
        "id": "ErR5lcd6JGQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster = LancasterStemmer()\n",
        "lancaster_stemmed_tokens = [lancaster.stem(token) for token in tokens]\n",
        "print(\"Lancaster Stemmed Tokens: \", lancaster_stemmed_tokens)"
      ],
      "metadata": {
        "id": "HnZ5ezKEJWop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question No.5\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n"
      ],
      "metadata": {
        "id": "4UWdZyt7MoMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Sentiment Dataset Urdu - Sentiment Dataset Urdu.csv', encoding='utf-8')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LxPi2mFiNJTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text = df['Text'].to_string(index=False, header=False).strip().replace('\\n', ' ')\n",
        "text = df['Text']\n",
        "print(text)"
      ],
      "metadata": {
        "id": "n_XhOSziNSQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer"
      ],
      "metadata": {
        "id": "v3p5qctbOhXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.train_from_iterator(text, trainer)\n",
        "vocab = tokenizer.get_vocab()\n",
        "print(\"Vocab: \", vocab)\n",
        "print(\"Vocab Size: \", len(vocab))"
      ],
      "metadata": {
        "id": "ofeaKtLGO6JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = tokenizer.encode(\"پچھلی بارشوں کے باعث شہر کی سڑکیں تالاب میں تبدیل ہو گئیں اور حکومتیں کچھ نہ کر سکیں۔ شہریوں کو اپنی املاک اور جانوں کا نقصان اٹھانا پڑا۔ زندگی مفلوج ہو گئی اور روزمرہ کی مشکلات میں اضافہ ہوا۔\")\n",
        "print(\"Output Tokens: \", output_tokens.tokens)\n",
        "print(\"Output Tokens Size: \", len(output_tokens))\n",
        "bpe_tokens = output_tokens.tokens"
      ],
      "metadata": {
        "id": "dumVTZT1VU_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = set()\n",
        "for key, value in vocab.items():\n",
        "  dictionary.add(key)"
      ],
      "metadata": {
        "id": "B-uowdMLmnfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_match_segment(text, dictionary):\n",
        "    segmented = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        for j in range(len(text), i, -1):\n",
        "            word = text[i:j]\n",
        "            for dic in dictionary:\n",
        "                if dic == word:\n",
        "                    segmented.append(word)\n",
        "                    i = j\n",
        "                    break\n",
        "        else:\n",
        "            segmented.append(text[i])\n",
        "            i += 1\n",
        "    return segmented\n"
      ],
      "metadata": {
        "id": "_t-yeZoIrb1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"پچھلی بارشوں کے باعث شہر کی سڑکیں تالاب میں تبدیل ہو گئیں اور حکومتیں کچھ نہ کر سکیں۔ شہریوں کو اپنی املاک اور جانوں کا نقصان اٹھانا پڑا۔ زندگی مفلوج ہو گئی اور روزمرہ کی مشکلات میں اضافہ ہوا۔\"\n",
        "segmented_words = max_match_segment(text, dictionary)\n",
        "print(\"Output Tokens:\", segmented_words)\n",
        "print(\"Output Tokens Size: \", len(segmented_words))\n",
        "mm_tokens = segmented_words"
      ],
      "metadata": {
        "id": "dv6JkayOsdjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "def train_lm(train_tokens, n):\n",
        "    train_data, padded_vocab = padded_everygram_pipeline(n, [train_tokens])\n",
        "    print(\"Training Padded Vocabulary:\", list(padded_vocab))\n",
        "    for data in train_data:\n",
        "        print(\"Training Data: \", list(data))\n",
        "    model = MLE(n)\n",
        "    model.fit(train_data, padded_vocab)\n",
        "    return model\n",
        "\n",
        "def compute_perplexity(model, test_tokens, n):\n",
        "    test_data, _ = padded_everygram_pipeline(n, [test_tokens])\n",
        "    total_log_prob = 0\n",
        "    num_tokens = 0\n",
        "\n",
        "    for data in test_data:\n",
        "        num_tokens += len(list(data))\n",
        "        for ngram in list(data):\n",
        "            prob = model.score(ngram[-1], *ngram[:-1])  # For n-grams\n",
        "            total_log_prob += np.log(prob)\n",
        "\n",
        "    # Calculate perplexity\n",
        "    entropy = -total_log_prob / num_tokens\n",
        "    perplexity = np.exp2(entropy)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "train_tokens = list(dictionary)\n",
        "print(\"Train Tokens: \", train_tokens)\n",
        "\n",
        "test_text = \"دو دن کی بارش میں سارا” بھرم“ بہہ گیا۔\"\n",
        "test_tokens = get_word_tokens(test_text)\n",
        "print(\"Test Tokens: \", test_tokens)\n",
        "\n",
        "# Train unigram model\n",
        "unigram_model = train_lm(train_tokens, 1)\n",
        "unigram_perplexity = compute_perplexity(unigram_model, test_tokens, 1)\n",
        "print(\"Unigram Model Perplexity:\", unigram_perplexity)\n",
        "\n",
        "# Train bigram model\n",
        "bigram_model = train_lm(train_tokens, 2)\n",
        "bigram_perplexity = compute_perplexity(bigram_model, test_tokens, 2)\n",
        "print(\"Bigram Model Perplexity:\", bigram_perplexity)\n",
        "\n",
        "# Train trigram model\n",
        "trigram_model = train_lm(train_tokens, 3)\n",
        "trigram_perplexity = compute_perplexity(trigram_model, test_tokens, 3)\n",
        "print(\"Trigram Model Perplexity:\", trigram_perplexity)\n"
      ],
      "metadata": {
        "id": "C1VttsPv_iq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}